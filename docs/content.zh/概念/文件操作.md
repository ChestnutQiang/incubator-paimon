---
title: "文件操作"
weight: 4
type: docs
aliases:
- /概念/文件操作.html
---
<!--
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
-->

# 文件操作

This article is specifically designed to clarify 
the impact that various file operations have on files. 

This page provides concrete examples and practical tips for 
effectively managing them. Furthermore, through an in-depth 
exploration of operations such as commit and compact, 
we aim to offer insights into the creation and updates of files.

## 前提条件

在深入研究本文之前，请确保你已经阅读了

1. [基础概念]({{< ref "概念/基础概念" >}})
2. [文件布局]({{< ref "概念/文件布局" >}})
3. 如何在[Flink]({{< ref "engines/flink" >}})中使用Paimon

## Create Catalog

通过`./sql-client.sh`启动Flink SQL客户端，并逐一执行以下语句来创建一个Paimon Catalog。

```sql
CREATE CATALOG paimon WITH (
'type' = 'paimon',
'warehouse' = 'file:///tmp/paimon'
);

USE CATALOG paimon;
```

这将只在给定的路径`file:///tmp/paimon`创建一个目录。

## Create Table

执行下面的创建表语句将创建一个有3个字段的Paimon表：

```sql
CREATE TABLE T (
  id BIGINT,
  a INT,
  b STRING,
  dt STRING COMMENT 'timestamp string in format yyyyMMdd',
  PRIMARY KEY(id, dt) NOT ENFORCED
) PARTITIONED BY (dt);
```

这将在路径`/tmp/paimon/default.db/T`下创建Paimon表`T`、其schema存储在`/tmp/paimon/default.db/T/schema/schema-0`。


## Insert Records Into Table

在Flink SQL中运行以下插入语句：

```sql
INSERT INTO T VALUES (1, 10001, 'varchar00001', '20230501');
```

一旦Flink工作完成，记录就会通过成功的`commit`写到Paimon表中。用户可以通过执行查询`SELECT * FROM T`来验证这些记录的可见性，该查询将返回一条记录。提交过程创建了一个snapshot，位于`/tmp/paimon/default.db/T/snapshot/snapshot-1`的路径。在 snapshot-1 处产生的文件布局如下所述：

{{< img src="/img/file-operations-0.png">}}

snapshot-1 的内容包含snapshot的元数据，如`清单列表(manifest list)` 和`schema id`：
```json
{
  "version" : 3,
  "id" : 1,
  "schemaId" : 0,
  "baseManifestList" : "manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0",
  "deltaManifestList" : "manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1",
  "changelogManifestList" : null,
  "commitUser" : "7d758485-981d-4b1a-a0c6-d34c3eb254bf",
  "commitIdentifier" : 9223372036854775807,
  "commitKind" : "APPEND",
  "timeMillis" : 1684155393354,
  "logOffsets" : { },
  "totalRecordCount" : 1,
  "deltaRecordCount" : 1,
  "changelogRecordCount" : 0,
  "watermark" : -9223372036854775808
}
```

请注意，`清单列表(manifest list)`包含snapshot的所有变化，`baseManifestList`是基础文件，`deltaManifestList`中是在基础文件上的变化。第一次提交将产生一个清单(manifest)文件，并创建两个清单列表文件（文件名可能与您实验中的不同）：

```bash
./T/manifest:
manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1	
manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0
manifest-2b833ea4-d7dc-4de0-ae0d-ad76eced75cc-0
```
`manifest-2b833ea4-d7dc-4de0-ae0d-ad76eced75cc-0`是 manifest 文件（上图中的 manifest-1-0），它存储了snapshot中数据文件的信息。

`manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0`是baseManifestList（上图中的 manifest-list-1-base），它实际上是空的。

`manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1`是指 deltaManifestList（上图中的manifest-list-1-delta），其中包含对数据文件进行操作的清单条目的列表文件的清单，在本例中，它是 "manifest-1-0"。



现在让我们在不同的分区插入一批记录，看看会发生什么。

看看会发生什么。在Flink SQL中，执行以下语句：

```sql
INSERT INTO T VALUES 
(2, 10002, 'varchar00002', '20230502'),
(3, 10003, 'varchar00003', '20230503'),
(4, 10004, 'varchar00004', '20230504'),
(5, 10005, 'varchar00005', '20230505'),
(6, 10006, 'varchar00006', '20230506'),
(7, 10007, 'varchar00007', '20230507'),
(8, 10008, 'varchar00008', '20230508'),
(9, 10009, 'varchar00009', '20230509'),
(10, 10010, 'varchar00010', '20230510');
```

第二次 "提交 "发生后，执行`SELECT * FROM T`将返回10条记录。常见了一个新的snapshot，即`napshot-2`，并给我们提供了以下的物理文件布局：
```bash
 % ls -atR . 
./T:
dt=20230501
dt=20230502	
dt=20230503	
dt=20230504	
dt=20230505	
dt=20230506	
dt=20230507	
dt=20230508	
dt=20230509	
dt=20230510	
snapshot
schema
manifest

./T/snapshot:
LATEST
snapshot-2
EARLIEST
snapshot-1

./T/manifest:
manifest-list-9ac2-5e79-4978-a3bc-86c25f1a303f-1	 # delta manifest list for snapshot-2
manifest-list-9ac2-5e79-4978-a3bc-86c25f1a303f-0  # base manifest list for snapshot-2	
manifest-f1267033-e246-4470-a54c-5c27fdbdd074-0	 # manifest file for snapshot-2

manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1	 # delta manifest list for snapshot-1 
manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0  # base manifest list for snapshot-1
manifest-2b833ea4-d7dc-4de0-ae0d-ad76eced75cc-0  # manifest file for snapshot-1

./T/dt=20230501/bucket-0:
data-b75b7381-7c8b-430f-b7e5-a204cb65843c-0.orc

...
# each partition has the data written to bucket-0
...

./T/schema:
schema-0
```
从snapshot-2开始，新的文件布局看起来像
{{< img src="/img/file-operations-1.png">}}

## 从表中删除记录(Delete Records From Table)

现在让我们删除符合条件`dt>=20230503`的记录。在Flink SQL中，执行以下语句：

{{< label Batch >}}
```sql
DELETE FROM T WHERE dt >= '20230503';
```
第三次`commit`发生了，它创建了`snapshot-3`。现在，列出表下面的所有文件，你会发现没有分区被删除。而是为分区`20230503`到`20230510`创建了新的数据文件：

```bash
./T/dt=20230510/bucket-0:
data-b93f468c-b56f-4a93-adc4-b250b3aa3462-0.orc # newer data file created by the delete statement 
data-0fcacc70-a0cb-4976-8c88-73e92769a762-0.orc # older data file created by the insert statement
```

这是是合情合理的，因为我们在第二次提交时插入了一条记录（用以下方式表示`+I[10, 10010, 'varchar00010', '20230510']`），然后在第三次提交中删除该记录。执行`SELECT * FROM T`将返回2条记录，即：
```
+I[1, 10001, 'varchar00001', '20230501']
+I[2, 10002, 'varchar00002', '20230502']
```

从snapshot-3开始，新的文件布局看起来像
{{< img src="/img/file-operations-2.png">}}

注意，`manifest-3-0`包含8个`ADD`操作类型的清单条目、对应于8个新写入的数据文件。



## 压实表(Compact Table)

正如你可能已经注意到的，小文件的数量会随着连续的snapshot，这可能导致读取性能下降。因此，需要进行全面 compact 来减少小文件的数量。

现在让我们触发全面compact，并通过 `flink run`运行一个专门的 compact 作业：

{{< label Batch >}}
```bash  
<FLINK_HOME>/bin/flink run \
    /path/to/paimon-flink-action-{{< version >}}.jar \
    compact \
    --warehouse <warehouse-path> \
    --database <database-name> \ 
    --table <table-name> \
    [--partition <partition-name>] \
    [--catalog-conf <paimon-catalog-conf> [--catalog-conf <paimon-catalog-conf> ...]] \
```

一个例子是(假设你已经在Flink home了)

```bash
./bin/flink run \
    ./lib/paimon-flink-action-{{< version >}}.jar \
    compact \
    --path file:///tmp/paimon/default.db/T
```

所有当前的表文件将被 compact，制作一个新的snapshot，即`snapshot-4`，并包含以下信息。并包含以下信息：

```json
{
  "version" : 3,
  "id" : 4,
  "schemaId" : 0,
  "baseManifestList" : "manifest-list-9be16-82e7-4941-8b0a-7ce1c1d0fa6d-0",
  "deltaManifestList" : "manifest-list-9be16-82e7-4941-8b0a-7ce1c1d0fa6d-1",
  "changelogManifestList" : null,
  "commitUser" : "a3d951d5-aa0e-4071-a5d4-4c72a4233d48",
  "commitIdentifier" : 9223372036854775807,
  "commitKind" : "COMPACT",
  "timeMillis" : 1684163217960,
  "logOffsets" : { },
  "totalRecordCount" : 38,
  "deltaRecordCount" : 20,
  "changelogRecordCount" : 0,
  "watermark" : -9223372036854775808
}
```

`manifest-4-0`
{{< img src="/img/file-operations-3.png">}}

注意，manifest-4-0包含20个清单项(18个DELETE操作和2个ADD操作)。

1. 对于 20230503 ~ 20230510 分区，对两个数据文件执行两次DELETE操作
2. 对于 20230501 ~ 20230502 分区，对同一个数据文件执行一次DELETE操作和一次ADD操作。


## 修改表(Alter Table)
执行以下语句配置全compact:
```sql
ALTER TABLE T SET ('full-compaction.delta-commits' = '1');
```

它将为Paimon表创建一个新的 schema，即`schema-1`，但是在下一次提交之前，还没有快照实际使用这个 schema。

## 过期snapshot(Expire Snapshots)

提醒大家，标记的数据文件不会被真正删除。 当snapshot过期并且没有消费者依赖该snapshot 后，数据文件才会被真正删除。更多信息请参见[Expiring Snapshots]({{< ref "maintenance/manage-snapshots#expiring-snapshots" >}})。

在snapshot过期的过程中，snapshot的范围被初步确定，然后这些snapshot中的数据文件被标记为要删除。
一个数据文件只有在清单条目存在引用该特定数据文件的`DELETE`类型的时，才会被标记为要删除。这种标记确保该文件不会被随后的snapshot所利用，可以安全地被删除。

假设上图中的4个snapshot都要过期了。过期的过程如下：

1.  它首先删除所有标记的数据文件，并记录每个改变的桶。 
2.  然后删除任何变化日志文件和相关清单。 
3.  最后，它删除snapshot本身并写入最早的提示(hint)文件。



如果在删除过程中，有任何目录是空的，它们也将被删除。



假设`snapshot-5`被创建，然后 snapshot 过期被触发。`snapshot-1`到`snapshot-4`将被删除。
为了简单起见，我们将只关注以前snapshot中的文件，snapshot过期后的最终布局看起来像：

{{< img src="/img/file-operations-4.png">}}

结果是分区`20230503`到`20230510`被物理删除。

## Flink Stream Write

最后，我们将通过利用CDC摄取的例子来研究Flink流写入。CDC摄取的例子。本节将讨论捕获和写入变化数据写入 Paimon，以及异步compact、snapshot提交和过期背后的机制。

首先，让我们仔细看看CDC的摄取工作流程和所涉及的每个组件所扮演的独特角色。

{{< img src="/img/cdc-ingestion-topology.png">}}

1. `MySQL CDC Source`统一读取snapshot和增量数据，由`SnapshotReader`读取snapshot数据
   和`BinlogReader`分别读取增量数据。
2.  `Paimon Sink`将数据写入桶级的Paimon表。其中的`CompactManager`将异步触发compact工作。
3. `Committer Operator`是一个单并行度算子，负责提交和过期的snapshot。

接下来，我们将介绍一下端到端的数据流。


{{< img src="/img/cdc-ingestion-source.png">}}

`MySQL Cdc Source`读取snapshot和增量数据，并在规范化后向下游发送。

{{< img src="/img/cdc-ingestion-write.png">}}


`Paimon Sink`首先在基于堆的LSM树中缓冲新记录，并在内存缓冲区满时将它们刷新到磁盘。注意，写入的每个数据文件都是经过`sorted run`。此时，还没有创建清单文件和快照。就在Flink检查点发生之前，`Paimon Sink`将刷新所有缓冲记录并向下游发送可提交的消息，由`Committer Operator`在检查点期间读取并提交。

{{< img src="/img/cdc-ingestion-commit.png">}}

在检查点期间，`Committer Operator`将创建一个新的snapshot，并将其与清单列表相关联，以便snapshot
包含表的所有数据文件的信息。

{{< img src="/img/cdc-ingestion-compact.png">}}

稍稍后可能会发生异步compact，由`CompactManager`生成的可提交文件包含有关先前文件和合并文件的信息，以便`Committer Operator`可以构造相应的清单项。在这种情况下，`Committer Operator`可能在Flink检查点期间生成两个快照，一个用于写入数据(类型为`Append`的快照)，另一个用于`compact`(类型为`compact`的快照)。如果在检查点时间间隔内没有写入数据文件，则只创建`Compact`类型的快照。`Committer Operator`将检查快照是否过期，并对标记的数据文件执行物理删除。
